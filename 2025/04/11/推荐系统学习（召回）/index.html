<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>推荐系统学习（召回） | YangLingSanShan</title><meta name="author" content="舲."><meta name="copyright" content="舲."><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="lpr的搜推入门（二）">
<meta property="og:type" content="article">
<meta property="og:title" content="推荐系统学习（召回）">
<meta property="og:url" content="https://yanglingsanshan.github.io/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8F%AC%E5%9B%9E%EF%BC%89/index.html">
<meta property="og:site_name" content="YangLingSanShan">
<meta property="og:description" content="lpr的搜推入门（二）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yanglingsanshan.github.io/img/avatar.png">
<meta property="article:published_time" content="2025-04-11T03:57:13.000Z">
<meta property="article:modified_time" content="2025-04-11T08:21:09.424Z">
<meta property="article:author" content="舲.">
<meta property="article:tag" content="推荐系统">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yanglingsanshan.github.io/img/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "推荐系统学习（召回）",
  "url": "https://yanglingsanshan.github.io/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8F%AC%E5%9B%9E%EF%BC%89/",
  "image": "https://yanglingsanshan.github.io/img/avatar.png",
  "datePublished": "2025-04-11T03:57:13.000Z",
  "dateModified": "2025-04-11T08:21:09.424Z",
  "author": [
    {
      "@type": "Person",
      "name": "舲.",
      "url": "https://yanglingsanshan.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/luoxiaohan_3.jpg"><link rel="canonical" href="https://yanglingsanshan.github.io/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8F%AC%E5%9B%9E%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '推荐系统学习（召回）',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/luoxiaohan_1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">YangLingSanShan</span></a><a class="nav-page-title" href="/"><span class="site-name">推荐系统学习（召回）</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">推荐系统学习（召回）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-11T03:57:13.000Z" title="发表于 2025-04-11 11:57:13">2025-04-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-11T08:21:09.424Z" title="更新于 2025-04-11 16:21:09">2025-04-11</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="召回"><a href="#召回" class="headerlink" title="召回"></a>召回</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>召回是推荐系统链路中的第一个流程，目的是<strong>从几亿的item中选出几千item</strong>。进行召回的模型方法是多种多样，包括但不仅限于基于统计学的、基于规则的和基于神经网络的。</p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><h3 id="协同过滤方法"><a href="#协同过滤方法" class="headerlink" title="协同过滤方法"></a>协同过滤方法</h3><h4 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h4><p>通过分析用户或者事物之间的相似性（“协同”），来预测用户可能感兴趣的内容并将此内容推荐给用户。协同过滤<strong>有泛化能力弱、热门物品头部效应强</strong>的弱点</p>
<h4 id="UserCF"><a href="#UserCF" class="headerlink" title="UserCF"></a>UserCF</h4><h5 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h5><p>如果用户 $user1$  与用户 $user2$ 相似，而且 $user2$ 喜欢某物品 $item1$ ，那么 $user1$ 很可能也喜欢该物品。根据该思想，UserCF的实现需要基于以下步骤:</p>
<ol>
<li>基于转化流程中的动作得到用户对某物品的兴趣分数 $ like(user_j,item) $ ;</li>
<li>离线计算得到的用户之间的相似度 $sim(user_j,user_i)$ ；</li>
<li>线上预估用户对候选物品的兴趣:$\sum_j(like(user_j, item) \times sim(user_j, user))$</li>
</ol>
<h6 id="用户相似度计算："><a href="#用户相似度计算：" class="headerlink" title="用户相似度计算："></a>用户相似度计算：</h6><p>主流的方法是<strong>Jaccard距离</strong>，UserCF认为两个用户喜欢的物品重合度越高，两个用户越相似。所以计算物品相似度的计算方式如下：</p>
<ol>
<li>设用户 $user_1$ 喜欢的物品集合为 $J_1$ ，用户 $user_2$ 喜欢的物品集合为 $J_2$ ;</li>
<li>定义这两个集合的交集 $I=J_1∩J_2$ ，那么简单计算两个用户的相似度为$sim(u1,u2)=\frac{|I|}{\sqrt{ |J1|⋅|J1|}}∈[0,1]$ ;</li>
<li>该公式表示该用户的相似度计算时并没有区别对待冷门和热门物品，给予了它们同样的权重来影响相似度。由于热门物品只占所有物品的很小部分，而且很容易出现不同用户有相同的热门物品重合度，故<strong>越热门的物品越无法反映用户的独特的兴趣</strong>，为了<strong>降低热门物品的权重</strong>，<strong>用户的相似度</strong>计算公式更新为：$\operatorname{sim}\left(user_{1}, user_{2}\right)=\frac{\sum_{i \in I} \frac{1}{\log \left(1+n_{i}\right)}}{\sqrt{\left|J_{1}\right| \cdot\left|J_{1}\right|}}$其中 $n_i$ 是喜欢物品 $i$ 的用户数量，能够反映物品的热门程度。</li>
</ol>
<p>除此之外，也可以用其它方法计算用户相似度：</p>
<ol>
<li><p>首先，利用“<strong>用户评价</strong>”转化为共现矩阵，用户作为行坐标，商品作为列坐标，喜欢设置为1，不喜欢设置为-1（这样可以得到用户向量$\textbf{x}$，类似生成了用户的embedding space上的映射）；</p>
</li>
<li><p>通过向量计算一些距离：</p>
<ul>
<li><p>欧式距离：</p>
<script type="math/tex; mode=display">
\begin{array}{l}d(\mathbf{x}, \mathbf{y})=\sqrt{\sum_{i}\left(x_{i}-y_{i}\right)^{2}} \\\operatorname{sim}(\mathbf{x}, \mathbf{y})=\frac{1}{1+d(\mathbf{x}, \mathbf{y})}\end{array}</script></li>
<li><p>皮尔逊相关系数：</p>
<script type="math/tex; mode=display">
\operatorname{sim}(\mathbf{x}, \mathbf{y})=\frac{\sum_{i}\left(x_{i}-\overline{\mathbf{x}}\right)\left(y_{i}-\overline{\mathbf{y}}\right)}{\sqrt{\sum_{i}\left(x_{i}-\overline{\mathbf{x}}\right)^{2}} \sqrt{\sum_{i}\left(y_{i}-\overline{\mathbf{y}}\right)^{2}}}</script></li>
<li><p>余弦相似度：</p>
<script type="math/tex; mode=display">
\operatorname{sim}(\mathbf{x}, \mathbf{y})=\frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \times\|\mathbf{y}\|}=\frac{\sum_{i} x_{i} y_{i}}{\sqrt{\sum_{i} x_{i}^{2}} \sqrt{\sum_{i} y_{i}^{2}}}</script></li>
<li><p>Tanimoto系数：</p>
<script type="math/tex; mode=display">
\operatorname{sim}(\mathbf{x}, \mathbf{y})=\frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|^{2}+\|\mathbf{y}\|^{2}-\mathbf{x} \cdot \mathbf{y}}=\frac{\sum_{i} x_{i} y_{i}}{\sqrt{\sum_{i} x_{i}^{2}}+\sqrt{\sum_{i} y_{i}^{2}}-\sum_{i} x_{i} y_{i}}</script></li>
</ul>
</li>
</ol>
<ol>
<li>然后对Top n的用户进行加权平均：$r_{u, p}=\frac{\sum_{i} w_{u, i} \cdot r_{i, p}}{\sum_{i} w_{u, i}}$，其中$w$为相似度，$r$为相似度</li>
</ol>
<h5 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h5><p>在工程和效果上都有一些缺陷，总结为以下两点。</p>
<ol>
<li><p><strong>计算和存储开销</strong>。UserCF需要计算和存储用户之间的相似度，在互联网企业，用户规模一般都比较庞大，导致计算和存储的开销非常大。</p>
</li>
<li><p><strong>稀疏用户效果不佳</strong>。用户的历史行为一般都是比较稀疏的，比如电商场景，有些用户可能一个月就几次购买行为，对于这些用户计算相似度一般都不太准确，因此UserCF不太适合用户行为稀疏的场景。</p>
</li>
</ol>
<h4 id="ItemCF："><a href="#ItemCF：" class="headerlink" title="ItemCF："></a>ItemCF：</h4><h5 id="定义：-1"><a href="#定义：-1" class="headerlink" title="定义："></a>定义：</h5><p>由于UserCF工程和效果上的缺陷，大多数互联网企业都选择ItemCF。ItemCF是基于物品相似度进行推荐的协同过滤算法。具体讲，通过<strong>计算Item之间的相似度</strong>，得到Item相似度矩阵，然后找到<strong>用户历史正反馈物品的相似物品进行排序和推荐</strong>。</p>
<h5 id="原理：-1"><a href="#原理：-1" class="headerlink" title="原理："></a>原理：</h5><p>ItemCF的基本思想在于：如果用户喜欢物品 $item_1$ ，而物品 $item_2$ 与物品 $item_1$ 相似，那么用户很可能喜欢物品 $item_2$ 。根据该思想，ItemCF的实现需要基于以下步骤：</p>
<ol>
<li>基于转化流程中的动作得到某用户对物品的兴趣分数 $like(user,item_i)$ ;</li>
<li>离线计算得到的物品之间的相似度 $sim(item_i,item_j)$ ；</li>
<li>如下图所示，线上预估用户对候选物品的兴趣 $∑_i(like(user,item_i)×sim(item_i,item))$ ;</li>
</ol>
<h6 id="物品相似度计算："><a href="#物品相似度计算：" class="headerlink" title="物品相似度计算："></a><strong>物品相似度计算</strong>：</h6><p>一般来说，ItemCF的思想认为两个物品的受众重合度越高，两个物品越相似。所以计算物品相似度是基于与两个物品交互的用户群体重合度的：</p>
<ol>
<li>. 设喜欢物品 $item_1$ 的用户群体为集合 $W_1$ ，喜欢物品 $item_2$ 的用户群体为集合 $W_2$；</li>
<li>定义这两个集合的交集 $V=W1∩W2$ ，那么简单计算两个物品的相似度为$sim(item_1,item_2)=\frac{|V|}{\sqrt{|W1|⋅|W1|}}∈[0,1]$ ;</li>
<li><strong>考虑到用户对于不同物品的喜欢程度</strong>对相似度的影响：$\operatorname{sim}\left(i_{1}, i_{2}\right)=\frac{\sum_{v \in V}\left(\text { like }\left(v, i_{1}\right) \cdot l i k e\left(v, i_{2}\right)\right)}{\sqrt{\sum_{w_{1} \in W_{1}} l i k e^{2}\left(w_{1}, i_{1}\right)} \cdot \sqrt{\sum_{w_{2} \in W_{2}} l i k e^{2}\left(w_{2}, i_{2}\right)}} \in[0,1]$，（<strong>余弦相似度</strong>的表达形式）</li>
</ol>
<p><strong>相似地，也可以使用共现矩阵的列向量，计算物品之间的相似度</strong></p>
<h4 id="与UserCF对比："><a href="#与UserCF对比：" class="headerlink" title="与UserCF对比："></a>与UserCF对比：</h4><p>和UserCF相比，<strong>由于物品规模一般远远小于用户数</strong>，因此ItemCF的计算和存储开销都比UserCF小得多。除了技术实现上的区别，UserCF和ItemCF的应用场景也有所不同。总结为下面两点。</p>
<ol>
<li><p>UserCF更适合<strong>新闻推荐场景</strong>。在新闻推荐场景中，新闻的兴趣点一般比较分散，比如虎嗅的新闻一般受众群体是从事IT的人，而UserCF可以快速找到都是从事IT的人群，然后把虎嗅的新闻推荐给自己。</p>
</li>
<li><p>ItemCF更适合<strong>电商或视频推荐场景</strong>。在电商和视频推荐场景中，都有一个共同点，用户的兴趣比较稳定。比如在电商场景，ItemCF可以推荐和兴趣点相似的商品，比如以前经常购买球鞋的人，可以推荐球衣球裤。</p>
</li>
</ol>
<h3 id="矩阵分解方法："><a href="#矩阵分解方法：" class="headerlink" title="矩阵分解方法："></a>矩阵分解方法：</h3><p>协同过滤具有简单、可解释性强的优点，在推荐系统早期广泛应用。但是协同过滤也有泛化能力弱、热门物品头部效应强的弱点。为了解决上述问题，后来矩阵分解技术被提出来。</p>
<h4 id="SVD算法"><a href="#SVD算法" class="headerlink" title="SVD算法"></a>SVD算法</h4><p>SVD是Singlular Value Decomposition的简称，翻译过来就是<strong>奇异值分解</strong>。SVD的具体原理是假设矩阵 $M$的大小为 $n*m$，则可以分解成3个矩阵的乘法 $\textbf{M}=\textbf{U}\Sigma\textbf{V}^T$ ，其中 $\textbf{U}$ 是 $ m \times m$ 的正交矩阵， $\textbf{V}$ 是 $ n\times n$ 的正交矩阵， !$\Sigma$ 是 $m\times n$ 的对角矩阵。只取对角矩阵 $\Sigma$ 中的前 k 个较大元素，则SVD可以近似的表示为：</p>
<script type="math/tex; mode=display">
\mathbf{M} \approx \mathbf{U}_{m \times k} \Sigma_{k \times k} \mathbf{V}_{k \times n}^{T}</script><p>SVD应用在推荐系统中，用户矩阵表示为 $\textbf{U}_{m\times k}$，物品矩阵表示为 $\textbf{V}_{n\times k}$。这样就可以计算用户和物品的相似度，但是在工程上会有问题，奇异值分解的计算复杂度达到了 $O\left( mn^2 \right)$ ，对于物品上百万的互联网行业是无法接受的。另外，SVD对于稀疏的共现矩阵也不友好，需要人为填充缺失值，而互联行业用户行为大多是比较稀疏的。</p>
<p>针对传统SVD计算复杂度以及用户稀疏行为效果不好的问题，后来提出了改进的版本。用户 u 的向量表示为 $\textbf{p}_u$ ，物品 i 的的向量表示为 $\textbf{q}_i$，则用户对物品的评分表示为两个向量的内积 $\textbf{p}_u\cdot \textbf{q}_i$，真实的评分假设为 $r_{u,i}$，目标函数表示为：</p>
<script type="math/tex; mode=display">
\min _{\mathbf{p}^{*}, \mathbf{q}^{*}} \sum_{(u, i) \in K}\left(r_{u, i}-\mathbf{p}_{u} \cdot \mathbf{q}_{i}\right)^{2}</script><p>然后可以用梯度下降求解上面的目标函数。</p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>传统召回算法有<strong>其简单、可解释性强</strong>的特点，但是也有自身的局限性。协同过滤和矩阵分解都没有加入用户、物品和上下文相关的特征，也没有考虑用户行为之间的相关性。随着embedding技术的发展，召回技术开始朝着模型化embedding的方向演化。</p>
<hr>
<h2 id="Embedding-方法"><a href="#Embedding-方法" class="headerlink" title="Embedding 方法"></a>Embedding 方法</h2><p>基于embedding的召回框架，主要分为<strong>i2i</strong>召回和<strong>u2i</strong>召回。</p>
<p>i2i召回是指我们可以得到item embedding，但是模型没有直接得到user embedding。<strong>离线根据用户历史行为训练召回模型</strong>，输出item embedding，存储到数据库中，<strong>线上用户请求，根据用户历史行为，从数据库从查找对应的embedding</strong>，然后检索相似度最高的n个item，最后将Top n召回结果返回给后面的排序模块。</p>
<p><img src="\img\image-20250411101306849.png" alt="image-20250411101306849" style="zoom:50%;" /></p>
<p>u2i是指模型同时得到了user embedding和item embedding。</p>
<p>在u2i召回框架中，有时考虑到用户规模太大，不方便存储，可以在线上召回的时候，直接通过模型请求获取user embedding，然后再检索相似item。图10是u2i的召回框架，和图9的i2i相比，主要区别在于u2i可以直接基于user embedding进行检索。</p>
<p><img src="\img\image-20250411101424943.png" alt="image-20250411101424943" style="zoom:50%;" /></p>
<h3 id="基于内容语义的i2i召回"><a href="#基于内容语义的i2i召回" class="headerlink" title="基于内容语义的i2i召回"></a><strong>基于内容语义的i2i召回</strong></h3><h4 id="词向量方法"><a href="#词向量方法" class="headerlink" title="词向量方法"></a>词向量方法</h4><p>经典的词向量方法如下，不详细展开了，论文都十分经典。</p>
<ul>
<li><p>Word2vec</p>
<p>Word2vec是Google2013年在论文<em>Efficient Estimation of Word Representations in Vector Space</em>中提出的语言模型，用于生成词的Embedding。Word2vec有两种模型结构，<strong>CBOW是给定周围词预测中心词</strong>，而<strong>Skip-gram是给定中心词预测周围词</strong>。比如句子“we soon believe what he desire”，如果中心词是”believe”，窗口大小是2，则CBOW是用”we”,”soon”,”what”,”he”来预测”believe”；而Skip-gram是用中心词”believe”来预测”we”,”soon”,”what”,”he”。</p>
</li>
<li><p>FastText</p>
<p>FastText的输入是整个文本的词序列，同时在表示单个词Embeddiing的时候，引入了单个词的n-gram特征，最后词的Embedding就可以用n-gram向量的均值表示。</p>
</li>
<li><p>Bert</p>
<p>Bert是动态词向量方法。假设有两个句子”I have an apple”, “I have an apple phone”，把这两个句子分别作为Bert的input，<strong>由于Bert的每个词会与上下文做复杂的Attention计算</strong>，那么两个句子中”apple”对应的Embedding是不一样的，即Bert是动态词向量方法。</p>
</li>
</ul>
<h4 id="Item2vec："><a href="#Item2vec：" class="headerlink" title="Item2vec："></a><strong>Item2vec</strong>：</h4><p>相比Word2vec利用“词序列”生成词的Embedding，Item2vec利用“新闻点击序列”生成新闻的Embedding。假设Item2vec中一个长度为 K 的用户历史点击序列为 $item_1,item_2,..,item_K$ ，则Item2vec的优化目标为：</p>
<script type="math/tex; mode=display">
\frac{1}{K} \sum_{i=1}^{K} \sum_{1 \leq j \leq K, j \neq i} \log p\left(\text { item }_{j} \mid \text { item }_{i}\right)</script><h3 id="基于Graph-Embedding的i2i召回："><a href="#基于Graph-Embedding的i2i召回：" class="headerlink" title="基于Graph Embedding的i2i召回："></a><strong>基于Graph Embedding的i2i召回</strong>：</h3><p>列模型有自身的局限性，在互联网场景下，用户的行为数据往往呈现的是图结构。Item2vec等序列模型更多的是捕捉用户行为序列之间的相关性，而图结构中的二度邻居关系没有考虑。。在面对图结构时，传统的序列Embedding方法表达能力有限，由此Graph Embedding成了新的研究方向，在召回中得以广泛应用。</p>
<p>Graph Embedding是一种对图结构中的节点进行Embedding化的方法，最终生成的节点Embedding一般包含图的全局结构信息以及邻居节点的局部相似度信息。常用的Graph Embedding方法：</p>
<h4 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a><strong>DeepWalk</strong></h4><p>DeepWalk是近年来第一个有影响力的大规模Graph Embedding方法，它的本质是在图结构上进行随机游走(<strong>它的本质是从一个节点出发，随机选择它的一个邻接点，再从这个邻接点出发到下一个节点，重复这个步骤然后记录下所经过的所有节点</strong>)，生成Item序列，然后将这些Item序列作为训练样本输入Skip-Gram进行训练，得到Item的Embedding。</p>
<h4 id="EGES"><a href="#EGES" class="headerlink" title="EGES"></a><strong>EGES</strong></h4><p>前面的DeepWalk有一个问题，就是单纯使用行为序列构建图结构，虽然可以生成Item的Embedding，但是如果遇到新的Item，或者出现次数少的Item，则Embedding效果会很差，这就是<strong>推荐系统常见的冷启动问题</strong>。<strong>为了使冷启动的Item也有效果不错的Embedding</strong>，2018年，阿里巴巴公布了在淘宝应用的Graph Embedding方法EGES，其主要思想就是在DeepWalk生成Embedding过程中引入Item的补充信息。</p>
<p>在阿里巴巴的淘宝场景，Item就是衣服、电器这些物品，可以引入的补充信息包含物品的类别、对应的商店、风格、颜色等。有了物品的多个补充信息，那么问题是如何融合物品的多个Embedding，使之形成物品最后的Embedding。最简单的方法是在Skip-gram网络中加入Avg pooling，比如对于Item v ，假设包含 n 个补充信息，加上自身的Item id，一共有 n+1 个特征，对应的Embedding表示为 $W_v^0,W_v^1,…,W_v^n∈R_d$ ，使用Avg pooling，再加权平均，最后Item v 的Embedding表示为：</p>
<script type="math/tex; mode=display">
\mathbf{H}_{v}=\frac{\sum_{j=0}^{n} e^{a_{v}^{j}} \mathbf{W}_{v}^{j}}{\sum_{j=0}^{n} e^{a_{v}^{j}}}</script><p>(这篇文章感觉挺好的，可以看一下，他给了DeepWalk中的图变为了有权图，然后引入side information，再使用Avg pooling最后进行加权平均)</p>
<p><img src="\img\image-20250411111049233.png" alt="image-20250411111049233" style="zoom:50%;" /></p>
<h4 id="Node2vec"><a href="#Node2vec" class="headerlink" title="Node2vec"></a><strong>Node2vec</strong></h4><p>该模型通过调整random walk权重的方法使得节点的Embedding向量更倾向于体现网络的同质性或结构性。</p>
<p>同质性指的是距离相近的节点的Embedding向量应接近，如图6所示，与节点 u 相连的节点 s1,s2,s3,s4 的Embedding向量应相似。为了使Embedding向量能够表达网络的同质性，需要让随机游走倾向于DFS，因为DFS更有可能通过多次跳转，到达远方的节点上，使游走序列集中在一个较大的集合内部，这就使得在一个集合内部的节点具有更高的相似性，从而表达图的同质性。</p>
<p>结构性指的是结构相似的节点的Embedding向量应相似。为了表达结构性，需要随机游走更倾向于BFS，因为BFS会更多的在当前节点的邻域中游走，相当于对当前节点的网络结构进行扫描，从而使得Embedding向量能够刻画当前节点邻域的结构信息。</p>
<p>相较于DeepWalk，Node2vec通过设计biased-random walk策略，能对图中节点的结构相似和同质性相似进行权衡，使得模型生成的Embedding更加灵活，实用性更强。Node2vec应用于新闻推荐场景，同质性相同的新闻很可能是同类别的新闻，比如都是娱乐类新闻，而结构性相同的新闻很可能是一些热点新闻。</p>
<h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a><strong>GCN</strong></h4><p>近年来，研究人员尝试将深度图模型和神经网络相结合，直接在图中提取拓扑图的空间特征，这一类方法统称为GNN（Graph Neural Networks）。<strong>GCN</strong>是一种能够<strong>直接作用于图</strong>并且利用其结构信息的<strong>卷积神经网络</strong>。</p>
<h4 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a><strong>GraphSAGE</strong></h4><p>GraphSAGE（Graph SAmple and aggreGatE）是基于空间域的方法，其思想与基于谱域的方法相反，是直接在图上定义卷积操作，对空间上相邻的节点进行运算。<strong>GraphSAGE学习当前节点的Embedding是通过其邻居节点的特征聚合得到</strong>，这样就可以通过邻居节点很方便的表示一个新节点。</p>
<hr>
<h2 id="双塔模型"><a href="#双塔模型" class="headerlink" title="双塔模型"></a>双塔模型</h2><p>双塔模型广泛应用在各种领域，就是NLP领域的 query 和 document，推荐领域的 user 和 item，多模态检索领域的CLIP等，都可以用双塔表示，分别把两个领域的特征编码成一个向量，然后向量相似度进行召回。</p>
<p>通过大量实践证明，适用于<strong>召回</strong>的双塔模型是先分别通过神经网络生成表征，再计算两个表征之间的相似度（兴趣分数），称之为<strong>后期融合</strong>，；而在<strong>精排和粗排</strong>中，则需要使用<strong>前期融合</strong>，也就是先融合物品和用户的特征向量，再通过神经网络输出兴趣分数。</p>
<p>其它公司都有各自提出的双塔模型，在此仅仅介绍两种。</p>
<h3 id="DSSM："><a href="#DSSM：" class="headerlink" title="DSSM："></a>DSSM：</h3><p>DSSM（Deep Structured Semantic Models ，深度语义模型）是2013年微软发表的一篇论文，本用于语义匹配，后被移植到推荐系统等各个场景，成为经典的双塔模型。广告推荐领域中使用的 DSSM 双塔模型是<strong>从广告维度为广告主推荐一定数量的人群，从数量上看是从数亿级别人群中找出百万级人群用于投放广告，所以是召回模型。</strong></p>
<h4 id="原理：-2"><a href="#原理：-2" class="headerlink" title="原理："></a>原理：</h4><p>DSSM 从下往上可以分为三层结构：<strong>输入层、表示层、匹配层</strong>，先将用户和物品各自输入输入层，然后二者通过各自的表示层，最后在匹配层进行匹配。</p>
<h5 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h5><p>模型训练分成两座不同的“塔”分别进行，其实也就是两个不同的神经网络。其中一座塔是用于生成 user embedding。输入用户特征训练数据，用户特征包括用户稠密特征和用户稀疏特征，其中用户稠密特征进行 one-hot 编码操作，用户稀疏特征进行 embedding 降维到低维空间(64 或者 32 维)，然后进行特征拼接操作。广告侧和用户侧类似。</p>
<p>最复杂的就是这块的特征工作。</p>
<h5 id="表示层"><a href="#表示层" class="headerlink" title="表示层"></a>表示层</h5><p><img src="\img\image-20250411113413521.png" alt="image-20250411113413521" style="zoom:50%;" /></p>
<p><strong>用 tanh 作为隐层和输出层的激活函数，</strong>最终输出一个 128 维的低纬语义向量。</p>
<h5 id="匹配层："><a href="#匹配层：" class="headerlink" title="匹配层："></a>匹配层：</h5><p>Query 和 Doc 的语义相似性可以用这<strong>两个语义向量(128 维) 的 cosine 距离(即余弦相似度)</strong> 来表示： 通过 <strong>softmax</strong> 函数可以把 Query 与正样本 Doc 的语义相似性转化为一个后验概率。</p>
<h4 id="在推荐领域的应用："><a href="#在推荐领域的应用：" class="headerlink" title="在推荐领域的应用："></a>在推荐领域的应用：</h4><p>DSSM双塔结构，两侧分别输入user特征和ad特征，经过DNN变换后分别产出user向量和ad向量。DSSM最大的特点是user侧和ad侧是独立的两个子网络，可以离线产出user embedding和ad embedding，召回时只需要计算二者的相似度。</p>
<h3 id="SENet双塔结构"><a href="#SENet双塔结构" class="headerlink" title="SENet双塔结构"></a>SENet双塔结构</h3><p>SENet模块由自动驾驶公司Momenta在2017年提出，在当时，是一种应用于图像处理的新型网络结构。它基于CNN结构，通过对特征通道间的相关性进行建模，对重要特征进行强化来提升模型准确率，<strong>本质上就是针对CNN中间层卷积核特征的Attention操作</strong>。</p>
<p>用户侧塔和Item侧塔在特征Embedding层上，各自加入一个SENet模块，两个SENet各自对User侧和Item侧的特征，进行动态权重调整：</p>
<ul>
<li>动态抑制User或者Item内的部分低频无效特征，甚至清除掉（如果权重为0的话）不重要甚至是噪音的特征</li>
<li>突显那些对高层User Embedding和Item Embedding的特征交叉起重要作用的特征</li>
</ul>
<p>通过SENet更有利于表达两侧的特征交互，避免单侧无效特征经过DNN双塔非线性融合时带来的噪声，同时又带有非线性的作用。</p>
<h3 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h3><p>但是双塔模型也存在严重的头部效应，不能很好地处理长尾数据，只对高点击物品的表征学的好。所以之后也引入了<strong>自监督学习</strong>的思想，使用多种不同的<strong>特征变换</strong>（Mask、Dropout、互补等），更好地<strong>处理长尾数据</strong>、更充分地<strong>学习用户和物品的表征信息</strong>。</p>
<hr>
<h2 id="其它知识"><a href="#其它知识" class="headerlink" title="其它知识"></a>其它知识</h2><h3 id="特征变换方法："><a href="#特征变换方法：" class="headerlink" title="特征变换方法："></a>特征变换方法：</h3><p>在召回中也常用到深度学习中的多种特征变换方法，如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特征变换</th>
<th>基本思想</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Mask</td>
<td>随机挑选一些离散特征，把它们遮住。</td>
<td>处理前物品的特征集合u={ID、类目}；处理后u={default}。</td>
</tr>
<tr>
<td>Dropout</td>
<td>随机丢弃特征中50%的值（仅对多值离散特征生效）。</td>
<td>处理前某特征的向量a=[1, 2]；处理后a=[1, 0] 。</td>
</tr>
<tr>
<td>互补特征</td>
<td>将若干个特征随机分为两组，两组都是物品表征且特征互不重复。因为表示的都是同一物品，所以鼓励这两个特征向量相似。</td>
<td>某物品具有ID、类目、关键词、城市4个特征，随机分成 {ID，default，关键词，default} 和 {default，类目，default，城市} 这两种物品表征。</td>
</tr>
<tr>
<td>Mask一组关联的特征</td>
<td>1. 使用互信息（MI，mutual information）衡量关联度，离线计算特征两两之间的关联矩阵，根据关联矩阵进行特征的随机Mask。 2. 它比以上三种方法效果都要好，但是比较复杂，实现难度大且不容易维护。</td>
<td>1. 设一共有k种特征，离线计算特征两两之间的MI，得到k×k的矩阵； 2. 随机选一个特征作为seed，找到该seed最相关的k/2种特征； 3. Mask该seed及其最相关的k/2种特征，保留其余k/2种特征。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="其它召回通道："><a href="#其它召回通道：" class="headerlink" title="其它召回通道："></a>其它召回通道：</h3><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><div class="table-container">
<table>
<thead>
<tr>
<th>召回通道</th>
<th>原理</th>
<th>索引</th>
<th>召回流程</th>
</tr>
</thead>
<tbody>
<tr>
<td>GeoHash召回</td>
<td>用户可能对附近发生的事感兴趣，根据用户定位的GeoHash取回该地点最新的若干篇笔记。</td>
<td>GeoHash→优质笔记列表（按时间倒排）</td>
<td>无个性化召回（所以选择推荐优质笔记）</td>
</tr>
<tr>
<td>同城召回</td>
<td>用户可能对同城发生的事情感兴趣。</td>
<td>城市→优质笔记列表（按时间倒排）</td>
<td>无个性化召回</td>
</tr>
<tr>
<td>作者召回</td>
<td>用户对关注的作者发布的笔记感兴趣。</td>
<td>用户→关注的作者 作者→发布的笔记（按时间倒排）</td>
<td>用户→关注的作者→最新的笔记</td>
</tr>
<tr>
<td>有交互的作者召回</td>
<td>如果用户对某笔记感兴趣（点赞、收藏、转发），那么用户可能对该作者的其他笔记也感兴趣。</td>
<td>1. 用户→有交互的作者 2. 作者→发布的笔记（按时间倒排）</td>
<td>用户→有交互的作者→最新的笔记</td>
</tr>
<tr>
<td>相似作者召回</td>
<td>如果用户喜欢（关注、交互）某作者，那么用户喜欢相似的作者（类似ItemCF）。</td>
<td>1. 用户→感兴趣的作者 2. 作者→相似作者 3. 作者→发布的笔记（按时间倒排）</td>
<td>用户→感兴趣的作者→相似作者→最新的笔记</td>
</tr>
</tbody>
</table>
</div>
<p>除此之外，还有一种被称为缓存召回的召回方法。在精排到重排的过程中，从几百篇笔记中只筛选了几十篇作为推荐结果，大部分精排结果并没有被曝光。因此，<strong>缓存召回</strong>希望复用前 n 次推荐精排但没有曝光的结果，将它们缓存起来，作为一条召回通道。</p>
<h3 id="曝光过滤"><a href="#曝光过滤" class="headerlink" title="曝光过滤"></a><strong>曝光过滤</strong></h3><p>在小红书和抖音的推荐系统中，为了避免重复推荐物品，如果用户看过某个物品，则系统不再把该物品曝光给该用户，这个思想称之为<strong>曝光过滤</strong>。一般来说，曝光过滤是在召回层实现的，它的基本流程如下：</p>
<ol>
<li><p>每个用户，记录已经曝光给他的物品。（小红书只召回1个月以内的笔记，因此只需要记录每个用户最近1个月的曝光历史）</p>
</li>
<li><p>对于每个召回的物品，判断它是否已经给该用户曝光过，排除掉曾经曝光过的物品。</p>
</li>
</ol>
<h3 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a><strong>Bloom Filter</strong></h3><p>如果一位用户看过 n 个物品，本次召回 r 个物品。如果暴力对比需要的时间复杂度为 o(nr) ，计算过于巨大。所以在实践中使用 Bloom Filter 来判断一个物品ID是否在已经曝光的物品集合中：</p>
<ol>
<li>如果判断为no，那么该物品一定不在集合中；</li>
<li>如果判断为yes，那么该物品很可能在集合中（可能误伤，错误判断未曝光物品为已曝光，将其过滤掉）。</li>
</ol>
<p>如下所示是<strong>Bloom Filter 的实现原理</strong>：</p>
<blockquote>
<ol>
<li>Bloom Filter 把物品集合表征为一个 m 维的二进制向量；</li>
<li>每个用户有一个曝光物品的集合，表征为一个向量，需要 m bit的存储空间；</li>
<li>Bloom filter 有 k 个哈希函数，每个哈希函数把物品ID映射成介于 0 和 m−1 之间的整数；</li>
<li>已曝光物品ID通过哈希函数映射进了二进制向量中对应的位置，如果该位置为0则调整为1，若为1无需调整；</li>
<li>对于召回的物品ID，通过哈希函数映射，若对应位置全为1，则说明该物品已曝光，否则未曝光。</li>
</ol>
</blockquote>
<h2 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a>总结：</h2><p>召回的目的在于快速从几亿的物品中初步筛选出几千物品。无论是基于统计学、基于规则还是基于神经网络的召回方法都是行之有效的手段。协同过滤就是一种基础和经典的模型思想。</p>
<p><strong>ItemCF</strong>和<strong>UserCF</strong>这两种协同过滤的介绍，它们使用物品和用户的ID<strong>计算相似度</strong>，结合用户对物品的交互信息，来计算<strong>用户对物品的兴趣分数</strong>进行排序作为召回结果。但在工业实践中初始物品数量都以亿为单位，协同过滤使用的稀疏向量过于庞大和稀疏，计算复杂度过高。</p>
<p>因此，引入了使用embedding处理离散特征（比如物品类别）的<strong>矩阵补全</strong>模型。但是矩阵补全模型仅使用ID做embedding、负样本选取错误而且求内积做训练的方式（回归任务）不对，在实际应用中效果不佳。</p>
<p>针对这些问题，前人在矩阵补全的基础上改进得到了<strong>双塔模型</strong>。双塔模型使用<strong>用户和物品的多维特征</strong>属性进行embedding后，通过特征变化再输入神经网络（<strong>后期融合</strong>）通过计算得到<strong>余弦相似度</strong>；在负样本选取上，考虑到对<strong>简单负样本</strong>和<strong>困难负样本</strong>的混合选取；并将训练任务转化为分类问题，根据不同的训练方式推理了对应的<strong>交叉熵损失或合页损失</strong>。</p>
<p>但是双塔模型也存在严重的头部效应，不能很好地处理长尾数据，只对高点击物品的表征学的好。所以引入<strong>自监督学习</strong>的思想，使用多种不同的<strong>特征变换</strong>（Mask、Dropout、互补等），更好地<strong>处理长尾数据</strong>、更充分地<strong>学习用户和物品的表征信息</strong>。</p>
<blockquote>
<p>参考：<br>    [1] <a target="_blank" rel="noopener" href="https://www.zhihu.com/tardis/zm/art/351716045?source_id=1005">【总结】推荐系统——召回篇【1】</a></p>
<p>​       [2] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/678664853">推荐系统｜个人学习笔记 - 知乎</a></p>
<p>​       [3] <a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1156611">DSSM：深度语义匹配模型（及其变体CLSM、LSTM-DSSM）-腾讯云开发者社区-腾讯云</a></p>
<p>​       [4] <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI2MDU3OTgyOQ==&amp;mid=2247499063&amp;idx=1&amp;sn=d62d53a687c6f55b54037eabf4da0fbe&amp;scene=21#wechat_redirect">SENet双塔模型在推荐领域召回粗排的应用及其它</a></p>
<p>​       以及所有架构的原始论文</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://yanglingsanshan.github.io">舲.</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yanglingsanshan.github.io/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8F%AC%E5%9B%9E%EF%BC%89/">https://yanglingsanshan.github.io/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8F%AC%E5%9B%9E%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://yanglingsanshan.github.io" target="_blank">YangLingSanShan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88CTR%E9%A2%84%E4%BC%B0%EF%BC%89/" title="推荐系统学习（CTR预估）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">推荐系统学习（CTR预估）</div></div><div class="info-2"><div class="info-item-1">CTR预估FM 因子分解机 论文链接：Factorization Machines  介绍：作为逻辑回归模型LR的改进版，拟解决在稀疏数据的场景下模型参数难以训练的问题。并且考虑了特征的二阶交叉，弥补了逻辑回归表达能力差的缺陷。 原理： y=\omega_{0}+\sum_{i=1}^{n} \omega_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \omega_{i j} x_{i} x_{j}在线性模型的基础上添加了一个多项式(最后一项)，用于描述特征之间的二阶交叉。但是参数学习困难，因为对 $w_{ij}$ 进行更新时，求得的梯度对应为$x_i$、 $x_j$，当且仅当二者都非 0 时参数才会得到更新。因此引入辅助向量，使用内积：  \hat{y}(\mathbf{x}):=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i}...</div></div></div></a><a class="pagination-related" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%80%89%E6%8B%A9%E6%80%A7%E5%81%8F%E5%B7%AE%EF%BC%89/" title="推荐系统学习（选择性偏差）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">推荐系统学习（选择性偏差）</div></div><div class="info-2"><div class="info-item-1">推荐系统中的Selection bias推荐系统中的biasSelection bias：当用户能够自由地选择给哪些物品打分的时候，则评分数据不是随机丢失的（missing not at random, MNAR），观测到的交互数据的分布将不能代表整体数据的分布。（当用户拥有自由选择权的时候，更倾向于给自己喜欢的物品打分。） Conformity bias：用户的打分会倾向于和群体一致，即使群体的打分有时候和用户的判断是有区别的，用户的这种倾向将使得评分并不能准确反映用户的偏好。大部分人都有从众的倾向，当用户发现自己的判断与大众不一致时，很可能改变自己的评分，而让自己的评分向大众的评分靠拢。 Exposure bias：用户只会暴露在一部分的物品上，因此没有交互过的物品不一定是用户不喜欢的，还可能是用户没看到。用户和物品没有交互存在两种可能性：用户没看到物品、用户不喜欢物品，直接讲没有交互过的物品当作负样本（用户不喜欢）会引入偏差。 Position...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/04/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89/" title="推荐系统学习（Word2vec）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-15</div><div class="info-item-2">推荐系统学习（Word2vec）</div></div><div class="info-2"><div class="info-item-1">推荐系统学习（Word2vec） 参考文献 Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems 26 (2013). Mikolov, Tomas, et al. “Efficient estimation of word representations in vector space.” arXiv preprint arXiv:1301.3781 (2013)  CBoW &amp; Skip-gram模型架构2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language...</div></div></div></a><a class="pagination-related" href="/2025/04/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88CTR%E9%A2%84%E4%BC%B0%EF%BC%89/" title="推荐系统学习（CTR预估）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-09</div><div class="info-item-2">推荐系统学习（CTR预估）</div></div><div class="info-2"><div class="info-item-1">CTR预估FM 因子分解机 论文链接：Factorization Machines  介绍：作为逻辑回归模型LR的改进版，拟解决在稀疏数据的场景下模型参数难以训练的问题。并且考虑了特征的二阶交叉，弥补了逻辑回归表达能力差的缺陷。 原理： y=\omega_{0}+\sum_{i=1}^{n} \omega_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \omega_{i j} x_{i} x_{j}在线性模型的基础上添加了一个多项式(最后一项)，用于描述特征之间的二阶交叉。但是参数学习困难，因为对 $w_{ij}$ 进行更新时，求得的梯度对应为$x_i$、 $x_j$，当且仅当二者都非 0 时参数才会得到更新。因此引入辅助向量，使用内积：  \hat{y}(\mathbf{x}):=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i}...</div></div></div></a><a class="pagination-related" href="/2025/04/18/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%86%B7%E5%90%AF%E5%8A%A8%EF%BC%89/" title="推荐系统学习（冷启动）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-18</div><div class="info-item-2">推荐系统学习（冷启动）</div></div><div class="info-2"><div class="info-item-1">冷启动冷启动介绍冷启动：对于新注册的用户或者新入库的标的物, 该怎么给新用户推荐标的物让用户满意，怎么将新标的物分发出去，推荐给喜欢它的用户。 如果是新开发的产品，初期用户很少，用户行为也不多，常用的协同过滤、深度学习等依赖大量用户行为的算法不能很好的训练出精准的推荐模型, 怎么让推荐系统很好的运转起来，让推荐变得越来越准确，这个问题就是系统冷启动。 分类： 标的物冷启动：也称为“物品冷启动”或“内容冷启动”，指的是系统中新增了某个推荐对象（如商品、电影、文章等），但由于该物品还没有用户互动数据（点击、评分、购买等），导致系统无法判断哪些用户可能喜欢它。  用户冷启动：指的是系统中新增用户或活跃度极低的用户，由于其没有或几乎没有行为数据，系统难以判断该用户的兴趣偏好，难以进行个性化推荐。  系统冷启动： 整个推荐系统刚刚上线或处于早期阶段**，缺乏足够的用户数据和物品数据，导致推荐算法无法有效学习  冷启动挑战： 我们一般对新用户知之甚少，...</div></div></div></a><a class="pagination-related" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%80%89%E6%8B%A9%E6%80%A7%E5%81%8F%E5%B7%AE%EF%BC%89/" title="推荐系统学习（选择性偏差）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-11</div><div class="info-item-2">推荐系统学习（选择性偏差）</div></div><div class="info-2"><div class="info-item-1">推荐系统中的Selection bias推荐系统中的biasSelection bias：当用户能够自由地选择给哪些物品打分的时候，则评分数据不是随机丢失的（missing not at random, MNAR），观测到的交互数据的分布将不能代表整体数据的分布。（当用户拥有自由选择权的时候，更倾向于给自己喜欢的物品打分。） Conformity bias：用户的打分会倾向于和群体一致，即使群体的打分有时候和用户的判断是有区别的，用户的这种倾向将使得评分并不能准确反映用户的偏好。大部分人都有从众的倾向，当用户发现自己的判断与大众不一致时，很可能改变自己的评分，而让自己的评分向大众的评分靠拢。 Exposure bias：用户只会暴露在一部分的物品上，因此没有交互过的物品不一定是用户不喜欢的，还可能是用户没看到。用户和物品没有交互存在两种可能性：用户没看到物品、用户不喜欢物品，直接讲没有交互过的物品当作负样本（用户不喜欢）会引入偏差。 Position...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">舲.</div><div class="author-info-description">剪影的你轮廓太好看
凝住眼泪才敢细看</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YangLingSanShan"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">轨迹改变角落交错，寂寞城市又再探戈，天空闪过灿烂花火，和你不再为爱奔波。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E"><span class="toc-number">1.</span> <span class="toc-text">召回</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">传统方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.1.</span> <span class="toc-text">协同过滤方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%EF%BC%9A"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">定义：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#UserCF"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">UserCF</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%EF%BC%9A"><span class="toc-number">1.2.1.2.1.</span> <span class="toc-text">原理：</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%EF%BC%9A"><span class="toc-number">1.2.1.2.1.1.</span> <span class="toc-text">用户相似度计算：</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="toc-number">1.2.1.2.2.</span> <span class="toc-text">缺点：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ItemCF%EF%BC%9A"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">ItemCF：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%EF%BC%9A-1"><span class="toc-number">1.2.1.3.1.</span> <span class="toc-text">定义：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%EF%BC%9A-1"><span class="toc-number">1.2.1.3.2.</span> <span class="toc-text">原理：</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%89%A9%E5%93%81%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%EF%BC%9A"><span class="toc-number">1.2.1.3.2.1.</span> <span class="toc-text">物品相似度计算：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8EUserCF%E5%AF%B9%E6%AF%94%EF%BC%9A"><span class="toc-number">1.2.1.4.</span> <span class="toc-text">与UserCF对比：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="toc-number">1.2.2.</span> <span class="toc-text">矩阵分解方法：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SVD%E7%AE%97%E6%B3%95"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">SVD算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-number">1.2.3.</span> <span class="toc-text">总结：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding-%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">Embedding 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E8%AF%AD%E4%B9%89%E7%9A%84i2i%E5%8F%AC%E5%9B%9E"><span class="toc-number">1.3.1.</span> <span class="toc-text">基于内容语义的i2i召回</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%8D%E5%90%91%E9%87%8F%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">词向量方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Item2vec%EF%BC%9A"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">Item2vec：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8EGraph-Embedding%E7%9A%84i2i%E5%8F%AC%E5%9B%9E%EF%BC%9A"><span class="toc-number">1.3.2.</span> <span class="toc-text">基于Graph Embedding的i2i召回：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DeepWalk"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">DeepWalk</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EGES"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">EGES</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Node2vec"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">Node2vec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GCN"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">GCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GraphSAGE"><span class="toc-number">1.3.2.5.</span> <span class="toc-text">GraphSAGE</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">双塔模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DSSM%EF%BC%9A"><span class="toc-number">1.4.1.</span> <span class="toc-text">DSSM：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%EF%BC%9A-2"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">原理：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82"><span class="toc-number">1.4.1.1.1.</span> <span class="toc-text">输入层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A1%A8%E7%A4%BA%E5%B1%82"><span class="toc-number">1.4.1.1.2.</span> <span class="toc-text">表示层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8C%B9%E9%85%8D%E5%B1%82%EF%BC%9A"><span class="toc-number">1.4.1.1.3.</span> <span class="toc-text">匹配层：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E6%8E%A8%E8%8D%90%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">在推荐领域的应用：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SENet%E5%8F%8C%E5%A1%94%E7%BB%93%E6%9E%84"><span class="toc-number">1.4.2.</span> <span class="toc-text">SENet双塔结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A-1"><span class="toc-number">1.4.3.</span> <span class="toc-text">总结：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E7%9F%A5%E8%AF%86"><span class="toc-number">1.5.</span> <span class="toc-text">其它知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="toc-number">1.5.1.</span> <span class="toc-text">特征变换方法：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E5%8F%AC%E5%9B%9E%E9%80%9A%E9%81%93%EF%BC%9A"><span class="toc-number">1.5.2.</span> <span class="toc-text">其它召回通道：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.5.3.</span> <span class="toc-text"> </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%9D%E5%85%89%E8%BF%87%E6%BB%A4"><span class="toc-number">1.5.4.</span> <span class="toc-text">曝光过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bloom-Filter"><span class="toc-number">1.5.5.</span> <span class="toc-text">Bloom Filter</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A-2"><span class="toc-number">1.6.</span> <span class="toc-text">总结：</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/18/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%86%B7%E5%90%AF%E5%8A%A8%EF%BC%89/" title="推荐系统学习（冷启动）">推荐系统学习（冷启动）</a><time datetime="2025-04-18T08:58:00.000Z" title="发表于 2025-04-18 16:58:00">2025-04-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89/" title="推荐系统学习（Word2vec）">推荐系统学习（Word2vec）</a><time datetime="2025-04-15T09:27:44.000Z" title="发表于 2025-04-15 17:27:44">2025-04-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%80%89%E6%8B%A9%E6%80%A7%E5%81%8F%E5%B7%AE%EF%BC%89/" title="推荐系统学习（选择性偏差）">推荐系统学习（选择性偏差）</a><time datetime="2025-04-11T07:55:24.000Z" title="发表于 2025-04-11 15:55:24">2025-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8F%AC%E5%9B%9E%EF%BC%89/" title="推荐系统学习（召回）">推荐系统学习（召回）</a><time datetime="2025-04-11T03:57:13.000Z" title="发表于 2025-04-11 11:57:13">2025-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88CTR%E9%A2%84%E4%BC%B0%EF%BC%89/" title="推荐系统学习（CTR预估）">推荐系统学习（CTR预估）</a><time datetime="2025-04-09T06:46:44.000Z" title="发表于 2025-04-09 14:46:44">2025-04-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 舲.</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>