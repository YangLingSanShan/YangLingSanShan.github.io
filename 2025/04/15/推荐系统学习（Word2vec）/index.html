<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>推荐系统学习（Word2vec） | YangLingSanShan</title><meta name="author" content="舲."><meta name="copyright" content="舲."><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="lpr的搜推入门（四）">
<meta property="og:type" content="article">
<meta property="og:title" content="推荐系统学习（Word2vec）">
<meta property="og:url" content="https://yanglingsanshan.github.io/2025/04/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89/index.html">
<meta property="og:site_name" content="YangLingSanShan">
<meta property="og:description" content="lpr的搜推入门（四）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yanglingsanshan.github.io/img/avatar.png">
<meta property="article:published_time" content="2025-04-15T09:27:44.000Z">
<meta property="article:modified_time" content="2025-04-18T07:47:00.529Z">
<meta property="article:author" content="舲.">
<meta property="article:tag" content="推荐系统">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yanglingsanshan.github.io/img/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "推荐系统学习（Word2vec）",
  "url": "https://yanglingsanshan.github.io/2025/04/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89/",
  "image": "https://yanglingsanshan.github.io/img/avatar.png",
  "datePublished": "2025-04-15T09:27:44.000Z",
  "dateModified": "2025-04-18T07:47:00.529Z",
  "author": [
    {
      "@type": "Person",
      "name": "舲.",
      "url": "https://yanglingsanshan.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/luoxiaohan_3.jpg"><link rel="canonical" href="https://yanglingsanshan.github.io/2025/04/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '推荐系统学习（Word2vec）',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg" style="background-image: url(/img/luoxiaohan_1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">YangLingSanShan</span></a><a class="nav-page-title" href="/"><span class="site-name">推荐系统学习（Word2vec）</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">推荐系统学习（Word2vec）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-15T09:27:44.000Z" title="发表于 2025-04-15 17:27:44">2025-04-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-18T07:47:00.529Z" title="更新于 2025-04-18 15:47:00">2025-04-18</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="推荐系统学习（Word2vec）">推荐系统学习（Word2vec）</h1>
<blockquote>
<p>参考文献</p>
<p><a target="_blank" rel="noopener" href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1310.4546">Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” <em>Advances in neural information processing systems</em> 26 (2013).</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1301.3781">Mikolov, Tomas, et al. “Efficient estimation of word representations in vector space.” <em>arXiv preprint arXiv:1301.3781</em> (2013)</a></p>
</blockquote>
<h3 id="CBoW-Skip-gram模型架构">CBoW &amp; Skip-gram模型架构</h3>
<p>2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language model[3]。在这篇文章里，他们总结出了一套用神经网络建立统计语言模型的框架（Neural Network Language Model，以下简称NNLM），并首次提出了<strong>word embedding</strong>的概念，从而奠定了包括word2vec在内后续研究word representation learning的基础。</p>
<p>NNLM模型的基本思想可以概括如下：</p>
<ol>
<li>假定词表中的每一个word都对应着一个连续的特征向量；</li>
<li>假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率；</li>
<li><strong>同时学习</strong>词向量的权重和概率模型里的参数。</li>
</ol>
<img src="/img/image-20250415000000001.png" alt="image-20250415000000001" style="zoom: 67%;" />
<p>我们可以将整个模型拆分成两部分加以理解：</p>
<ol>
<li>
<p>首先是一个线性的Embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的词向量（distributed vector）。其中，V是词典的大小，D是Embedding向量的维度（一个先验参数）。<strong>C矩阵里存储了要学习的word vector。</strong></p>
</li>
<li>
<p>其次是一个简单的前向反馈神经网络g。它由一个tanh隐层和一个softmax输出层组成。通过将Embedding层输出的N−1个词向量映射为一个长度为V的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估：<br>
$$<br>
p\left(w_{i} \mid w_{1}, w_{2}, \ldots, w_{t-1}\right) \approx f\left(w_{i}, w_{t-1}, \ldots, w_{t-n+1}\right)=g\left(w_{i}, C\left(w_{t-n+1}\right), \ldots, C\left(w_{t-1}\right)\right)<br>
$$</p>
</li>
</ol>
<p>值得注意的一点是，这里的<strong>词向量也是要学习的参数。<strong>在03年的论文里，Bengio等人采用了一个简单的前向反馈神经网络来拟合一个词序列的条件概率$p\left(w_{t} \mid w_{1}, w_{2}, \ldots, w_{t-1}\right)$。但是NNLM存在只能处理定长序列，缺少灵活性的问题，且</strong>训练过慢</strong>。</p>
<p>所以Word2vec对原始的NNLM模型做如下改造：</p>
<ol>
<li>移除前向反馈神经网络中非线性的hidden layer，直接将中间层的Embedding layer与输出层的softmax layer连接；</li>
<li>忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个Embedding layer；</li>
<li>将Future words纳入上下文环境</li>
</ol>
<p>这样就得到了CBoW模型，将其翻转，也就得到了Skip-gram模型：</p>
<img src="/img/image-20250415114926607.png" alt="image-20250415114926607" style="zoom:67%;" />
<h3 id="损失函数"><strong>损失函数</strong></h3>
<p>Skip-gram是通过中心词来推断上下文一定窗口内的单词，中心词center word就是指所给定的词，也就是<strong>模型所输入的词</strong>；上下文一定窗口内的单词叫做context words，窗口是指window size，也就是指定一个window size的值，所要预测的词语就是和中心词距离在window size内的所有其他词。</p>
<p>在这个模型中，每个词被表示为两个$d$维向量，用于计算条件概率。即假设一个词在词典中的索引为$i$，当它被当作中心词向量表示为$v_i ∈ R^d $，而为背景词时候向量变送hi为$u_i ∈ R^d $ ,设中心词$w_c$在词典中索引为$c$，背景词$w_o$ 在词典中索引为$o$，给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到：<br>
$$<br>
P\left(w_{o} \mid w_{c}\right)=\frac{\exp \left(\boldsymbol{u}<em>{o}^{\top} \boldsymbol{v}</em>{c}\right)}{\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}<em>{i}^{\top} \boldsymbol{v}</em>{c}\right)}<br>
$$<br>
由中心词来推断背景词，注意整个过程的输入是(中心词向量，背景词向量)，根据上面的所提到的背景词之间都是独立的，这种时候，可以把概率写成连乘的形式：<br>
$$<br>
\prod_{t=1}^{T} \prod_{-m \leq j \leq m, j=0} P\left(w^{(t+j)} \mid w^{(t)}\right)<br>
$$</p>
<p><strong>极大化似然估计</strong>的方式，对函数进行变换，从而改变函数的增减性，最终目的是最小化这个函数：<br>
$$<br>
-\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)<br>
$$<br>
当然，也可以展开，改写为：<br>
$$<br>
\log P\left(w_{o} \mid w_{c}\right)=\boldsymbol{u}<em>{o}^{\top} \boldsymbol{v}</em>{c}-\log \left(\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}<em>{i}^{\top} \boldsymbol{v}</em>{c}\right)\right)<br>
$$</p>
<h3 id="Hierarchical-Softmax">Hierarchical Softmax</h3>
<p>使用softmax计算损失，并反向传播更新梯度的过程本质上是一个极大似然估计。在使用softmax计算每个输出的时候，对于每一对样本的时间复杂度为<strong>O(V)</strong>。在实际情况中，可能遇到的单词单词个数可能非常多，可能高达几十万，所以softmax在计算输出的时候花费的时间复杂度通常是非常高的，为了降低在求取损失函数的的巨大复杂度，Hierarchical Softmax被提出，Hierarchical Softmax可以将时间复杂度降低到<strong>O(log(v))</strong>。</p>
<p><strong>hierarchical softmax替换的是hidden layer 到 output layer的全连接层部分。</strong></p>
<p><strong>根据词频构建哈夫曼树</strong>。如下图所示，假设在这句话中&quot;<strong>To become prepared for or as if for</strong>选取**“or”**作为输出，选取其他字符作为输入，求解过程如下：</p>
<img src="/img/image-20250415161357332.png" alt="image-20250415161357332" style="zoom:50%;" />
<ol>
<li>使用哈夫曼树的贪心策略，在一个自然语言处理数据集中，将每个词的one-hot编码与哈夫曼编码计算出来。</li>
<li>将{to, become, prepared, for, as ,if, for}的one-hot编码与矩阵 $W_{vn}$ 相乘求平均值,得到 $X_w=\frac{1}{C}W_{xn}(X1+X2+…Xn)$；</li>
<li>对于从根节点到目标词汇&quot;<strong>or</strong>&quot;, 有以下路径: $p_1^w,p_2^w,p_3^w,p_4^w,p_5^w$······</li>
<li>路径上的编码有: $d_2^w,d_3^w,d_4^w,d_5^w$······ ; $d_j^w$ 只能取值0或1，路径上的编码连起来也就是每个词汇的哈夫曼编码。</li>
<li>非叶子节点对应的参数向量： $θ_1^w,θ_2^w,θ_3^w,θ_4^w$</li>
<li>$l^w$ 代表路径中节点的数量。</li>
</ol>
<p>对于任意一个节点 $θ_{j−1}^w$ 而言，选择左边之路或者右边支路概率为：<br>
$$<br>
p\left(d_{j}^{w} \mid X_{w}, \theta_{j-1}^{w}\right) = \left{\begin{array}{ll}\sigma\left(X^{T} \theta_{j-1}^{w}\right), &amp; d_{j}^{w} = 0 \1-\sigma\left(X^{T} \theta_{j-1}^{w}\right), &amp; d_{j}^{w} = 1\end{array}\right.<br>
$$<br>
于是可以给出条件概率：<br>
$$<br>
p(w \mid \operatorname{Context}(w))=\prod_{j=2}^{l^{w}} p\left(d_{j}^{w} \mid \mathbf{x}<em>{w}, \theta</em>{j-1}^{w}\right)<br>
$$<br>
同样取对数：<br>
$$<br>
l(w)=\sum_{j=2}^{l_{w}}\left{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(X^{T} \theta_{j-1}^{w}\right)\right] \cdot d_{j}^{w} \log \cdot\left[1-\sigma\left(X^{T} \theta_{j-1}^{w}\right)\right]\right}<br>
$$<br>
这个就是Hierarchical Softmax的优化函数，目标是最大化这个函数。</p>
<p>hierarchical softmax设计的初衷是为了用hierarchical的binomial distribution去实现multinomial distribution。这是一种层级实现的softmax，所以叫hierarchical softmax.</p>
<h3 id="负采样">负采样</h3>
<p>对于机器学习中的分类任务，在训练的时候不但要给正例，还要给负例。对于Hierarchical Softmax，负例是二叉树的其他路径。对于Negative Sampling，负例是随机挑选出来的。利用 (相对简单的) <strong>随机负采样</strong>，能大幅度提高性能。<strong>用编造的噪声词汇训练的方法被称为Negative Sampling (NEG)</strong>。</p>
<p>具体来说，对于每一个输入，除了本身的 label（正样本），同时采样出N个其他 label（负样本），从而我们只需要计算样本在这N+1个 label 上的概率，而不用计算样本在所有 label 上的概率。而在N+1个 label 上计算概率时，不再用 Softmax，而是针对每个 label ，采用逻辑回归的 sigmoid。设计优化目标时，我们希望输出的正样本概率尽可能高，而负样本概率尽可能低。</p>
<p>在 CBOW 中已知词 $w$ 的上下文 $context(w)$ ，需要预测 $w$ ，因此$(context(w),w)$ 就是一个正样本，$context(w)$ 和 $w$ 外的其它词构成负样本。假设已经选取了 $m$ 个负样本$(w,u)$，其中 $u∈NEG(w)={u_1,u_2,…,u_m}$。</p>
<ul>
<li>
<p><strong>对于正样本</strong>，CBOW 模型的输出是：$p^w=σ(x_w^⊤θ^w)$</p>
<p>其中，$x_w∈R^{N×1}$ 是 上下文$context(w)$ 中所有单词的低维词向量的平均值；$θ_w∈R^{N×1}$ 是输出层中权重矩阵 $W^′∈R^{N×V}$ 中单词 $w$ 对应的上下文向量。$σ$ 即 sigmoid 函数。$p^w$ 为预测正样本的概率，我们希望 $p^w$ 越大越好。</p>
</li>
<li>
<p><strong>对于负样本</strong>，CBOW 模型的输出是：$p^u=σ(x_w^⊤θ^u)$，其中的符号与上面正样本类似。**其意义为负采样修改了原来的⽬标函数。给定中心词的⼀个背景窗口，我们把背景词出现在该背景窗口事件的概率。**我们希望$1-p^u$越大越好。</p>
</li>
</ul>
<p>因此，给定一个正样本$(context(w),w)$ 和它对应的负样本 $(context(w),u),u∈NEG(w)$ 后，它们的<strong>优化目标</strong>可设计为：<br>
$$<br>
L_w=p^w∏_{u∈NEG(w)}(1−p^u)<br>
$$</p>
<p>同样取对数，设文本序列中时间步 $t$ 的词 $w(t)$ 在词典中的索引为 $i_t$ ，噪声词 $w_k$ 在词典中的索引为 $h_k$ 。有关以上的<strong>对数损失</strong>为：<br>
$$<br>
\begin{array}{l}-\log P\left(w^{(t+j)} \mid w^{(t)}\right)=-\log P\left(D=1 \mid w^{(t)}, w^{(t+j)}\right)- \\sum_{k=1, w_{k} \sim P(w)}^{K} \log P\left(D=0 \mid w^{(t)}, w_{k}\right) \=-\log \sigma\left(u_{i_{t+j}}^{\top} v_{i_{t}}\right)-\sum_{k=1, w_{k} \sim P(w)}^{K} \log \left(1-\sigma\left(u_{h_{k}}^{\top} v_{i_{t}}\right)\right) \=-\log \sigma\left(u_{i_{t+j}}^{\top} v_{i_{t}}\right)-\sum_{k=1, w_{k} \sim P(w)}^{K} \log \sigma\left(-u_{h_{k}}^{\top} v_{i_{t}}\right)\end{array}<br>
$$<br>
现在，训练中每⼀步的梯度计算开销不再与词典大小相关，而与 采样数K线性相关。当K取较小的常数 时，负采样在每⼀步的梯度计算开销较小。</p>
<p>负采样在标准word2vec的基础上做了两点改进：</p>
<ol>
<li>
<p>针对softmax运算导致的每次梯度计算开销过⼤，将softmax函数调整为sigmoid函数，当然对应的含义也由给定中心词，每个词作为背景词的概率，变成了给定中心词，每个词出现在背景窗口中的概率</p>
</li>
<li>
<p>进行负采样，引入负样本，负采样的名字就是取了第二个改进点。</p>
</li>
</ol>
<h3 id="下采样">下采样</h3>
<p>除了层次Softmax和负采样的优化算法，Mikolov在13年的论文里还介绍了另一个trick：<strong>下采样（subsampling）。其基本思想是在训练时依概率随机丢弃掉那些高频的词：</strong><br>
$$<br>
p_{\text {discard }}(w)=1-\sqrt{\frac{t}{f(w)}}<br>
$$<br>
其中，$t$是一个先验参数，一般取为 $1e^{−5}$。$f(w)$是在语料中出现的频率。</p>
<h3 id="Item2vec的关系">Item2vec的关系</h3>
<p>本质上，word2vec模型是在word-context的co-occurrence矩阵基础上建立起来的。因此，任何基于co-occurrence矩阵的算法模型，都可以套用word2vec算法的思路加以改进。</p>
<p>协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果我们将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达，在更高阶上计算item间的相似度。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://yanglingsanshan.github.io">舲.</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yanglingsanshan.github.io/2025/04/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89/">https://yanglingsanshan.github.io/2025/04/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://yanglingsanshan.github.io" target="_blank">YangLingSanShan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%80%89%E6%8B%A9%E6%80%A7%E5%81%8F%E5%B7%AE%EF%BC%89/" title="推荐系统学习（选择性偏差）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">推荐系统学习（选择性偏差）</div></div><div class="info-2"><div class="info-item-1">推荐系统中的Selection bias 推荐系统中的bias Selection bias：当用户能够自由地选择给哪些物品打分的时候，则评分数据不是随机丢失的（missing not at random, MNAR），观测到的交互数据的分布将不能代表整体数据的分布。（当用户拥有自由选择权的时候，更倾向于给自己喜欢的物品打分。） Conformity bias：用户的打分会倾向于和群体一致，即使群体的打分有时候和用户的判断是有区别的，用户的这种倾向将使得评分并不能准确反映用户的偏好。大部分人都有从众的倾向，当用户发现自己的判断与大众不一致时，很可能改变自己的评分，而让自己的评分向大众的评分靠拢。 Exposure bias：用户只会暴露在一部分的物品上，因此没有交互过的物品不一定是用户不喜欢的，还可能是用户没看到。用户和物品没有交互存在两种可能性：用户没看到物品、用户不喜欢物品，直接讲没有交互过的物品当作负样本（用户不喜欢）会引入偏差。 Position...</div></div></div></a><a class="pagination-related" href="/2025/04/18/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%86%B7%E5%90%AF%E5%8A%A8%EF%BC%89/" title="推荐系统学习（冷启动）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">推荐系统学习（冷启动）</div></div><div class="info-2"><div class="info-item-1">冷启动 冷启动介绍 冷启动： 对于新注册的用户或者新入库的标的物, 该怎么给新用户推荐标的物让用户满意，怎么将新标的物分发出去，推荐给喜欢它的用户。 如果是新开发的产品，初期用户很少，用户行为也不多，常用的协同过滤、深度学习等依赖大量用户行为的算法不能很好的训练出精准的推荐模型, 怎么让推荐系统很好的运转起来，让推荐变得越来越准确，这个问题就是系统冷启动。 分类：   标的物冷启动：也称为“物品冷启动”或“内容冷启动”，指的是系统中新增了某个推荐对象（如商品、电影、文章等），但由于该物品还没有用户互动数据（点击、评分、购买等），导致系统无法判断哪些用户可能喜欢它。   用户冷启动：指的是系统中新增用户或活跃度极低的用户，由于其没有或几乎没有行为数据，系统难以判断该用户的兴趣偏好，难以进行个性化推荐。   **系统冷启动： 整个推荐系统刚刚上线或处于早期阶段，缺乏足够的用户数据和物品数据，导致推荐算法无法有效学习   冷启动挑战：  我们一般对新用户知之甚少，...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/04/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88CTR%E9%A2%84%E4%BC%B0%EF%BC%89/" title="推荐系统学习（CTR预估）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-09</div><div class="info-item-2">推荐系统学习（CTR预估）</div></div><div class="info-2"><div class="info-item-1">CTR预估 FM 因子分解机  论文链接：Factorization Machines  介绍： 作为逻辑回归模型LR的改进版，拟解决在稀疏数据的场景下模型参数难以训练的问题。并且考虑了特征的二阶交叉，弥补了逻辑回归表达能力差的缺陷。 原理： $$ y=\omega_{0}+\sum_{i=1}^{n} \omega_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \omega_{i j} x_{i} x_{j} $$ 在线性模型的基础上添加了一个多项式(最后一项)，用于描述特征之间的二阶交叉。但是参数学习困难，因为对 $w_{ij}$ 进行更新时，求得的梯度对应为$x_i$、 $x_j$，当且仅当二者都非 0 时参数才会得到更新。因此引入辅助向量，使用内积： $$ \hat{y}(\mathbf{x}):=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}{i}, \mathbf{v}{j}\right\rangle...</div></div></div></a><a class="pagination-related" href="/2025/04/18/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%86%B7%E5%90%AF%E5%8A%A8%EF%BC%89/" title="推荐系统学习（冷启动）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-18</div><div class="info-item-2">推荐系统学习（冷启动）</div></div><div class="info-2"><div class="info-item-1">冷启动 冷启动介绍 冷启动： 对于新注册的用户或者新入库的标的物, 该怎么给新用户推荐标的物让用户满意，怎么将新标的物分发出去，推荐给喜欢它的用户。 如果是新开发的产品，初期用户很少，用户行为也不多，常用的协同过滤、深度学习等依赖大量用户行为的算法不能很好的训练出精准的推荐模型, 怎么让推荐系统很好的运转起来，让推荐变得越来越准确，这个问题就是系统冷启动。 分类：   标的物冷启动：也称为“物品冷启动”或“内容冷启动”，指的是系统中新增了某个推荐对象（如商品、电影、文章等），但由于该物品还没有用户互动数据（点击、评分、购买等），导致系统无法判断哪些用户可能喜欢它。   用户冷启动：指的是系统中新增用户或活跃度极低的用户，由于其没有或几乎没有行为数据，系统难以判断该用户的兴趣偏好，难以进行个性化推荐。   **系统冷启动： 整个推荐系统刚刚上线或处于早期阶段，缺乏足够的用户数据和物品数据，导致推荐算法无法有效学习   冷启动挑战：  我们一般对新用户知之甚少，...</div></div></div></a><a class="pagination-related" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8F%AC%E5%9B%9E%EF%BC%89/" title="推荐系统学习（召回）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-11</div><div class="info-item-2">推荐系统学习（召回）</div></div><div class="info-2"><div class="info-item-1">召回 介绍 召回是推荐系统链路中的第一个流程，目的是从几亿的item中选出几千item。进行召回的模型方法是多种多样，包括但不仅限于基于统计学的、基于规则的和基于神经网络的。 传统方法 协同过滤方法 定义： 通过分析用户或者事物之间的相似性（“协同”），来预测用户可能感兴趣的内容并将此内容推荐给用户。协同过滤有泛化能力弱、热门物品头部效应强的弱点 UserCF 原理： 如果用户 $user1$  与用户 $user2$ 相似，而且 $user2$ 喜欢某物品 $item1$ ，那么 $user1$ 很可能也喜欢该物品。根据该思想，UserCF的实现需要基于以下步骤:  基于转化流程中的动作得到用户对某物品的兴趣分数 $ like(user_j,item) $ ; 离线计算得到的用户之间的相似度 $sim(user_j,user_i)$ ； 线上预估用户对候选物品的兴趣:$\sum_j(like(user_j, item) \times sim(user_j,...</div></div></div></a><a class="pagination-related" href="/2025/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%87%8D%E6%8E%92%E5%BA%8F%EF%BC%89/" title="推荐系统学习（重排序）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-22</div><div class="info-item-2">推荐系统学习（重排序）</div></div><div class="info-2"><div class="info-item-1">重排序  参考文献： ​		   [1] MMR 多样性算法 ​		   [2] Fast Greedy MAP Inference for Determinantal Point Process to Improve Recommendation Diversity 推荐系统｜个人学习笔记 - 知乎 ​		   推荐系统中的重排算法 -...</div></div></div></a><a class="pagination-related" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%80%89%E6%8B%A9%E6%80%A7%E5%81%8F%E5%B7%AE%EF%BC%89/" title="推荐系统学习（选择性偏差）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-11</div><div class="info-item-2">推荐系统学习（选择性偏差）</div></div><div class="info-2"><div class="info-item-1">推荐系统中的Selection bias 推荐系统中的bias Selection bias：当用户能够自由地选择给哪些物品打分的时候，则评分数据不是随机丢失的（missing not at random, MNAR），观测到的交互数据的分布将不能代表整体数据的分布。（当用户拥有自由选择权的时候，更倾向于给自己喜欢的物品打分。） Conformity bias：用户的打分会倾向于和群体一致，即使群体的打分有时候和用户的判断是有区别的，用户的这种倾向将使得评分并不能准确反映用户的偏好。大部分人都有从众的倾向，当用户发现自己的判断与大众不一致时，很可能改变自己的评分，而让自己的评分向大众的评分靠拢。 Exposure bias：用户只会暴露在一部分的物品上，因此没有交互过的物品不一定是用户不喜欢的，还可能是用户没看到。用户和物品没有交互存在两种可能性：用户没看到物品、用户不喜欢物品，直接讲没有交互过的物品当作负样本（用户不喜欢）会引入偏差。 Position...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">舲.</div><div class="author-info-description">剪影的你轮廓太好看
凝住眼泪才敢细看</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YangLingSanShan"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">轨迹改变角落交错，寂寞城市又再探戈，天空闪过灿烂花火，和你不再为爱奔波。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">推荐系统学习（Word2vec）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CBoW-Skip-gram%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">1.0.1.</span> <span class="toc-text">CBoW &amp; Skip-gram模型架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.0.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hierarchical-Softmax"><span class="toc-number">1.0.3.</span> <span class="toc-text">Hierarchical Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="toc-number">1.0.4.</span> <span class="toc-text">负采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E9%87%87%E6%A0%B7"><span class="toc-number">1.0.5.</span> <span class="toc-text">下采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Item2vec%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">1.0.6.</span> <span class="toc-text">Item2vec的关系</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%87%8D%E6%8E%92%E5%BA%8F%EF%BC%89/" title="推荐系统学习（重排序）">推荐系统学习（重排序）</a><time datetime="2025-04-22T03:47:00.000Z" title="发表于 2025-04-22 11:47:00">2025-04-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/18/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%86%B7%E5%90%AF%E5%8A%A8%EF%BC%89/" title="推荐系统学习（冷启动）">推荐系统学习（冷启动）</a><time datetime="2025-04-18T08:58:00.000Z" title="发表于 2025-04-18 16:58:00">2025-04-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88Word2vec%EF%BC%89/" title="推荐系统学习（Word2vec）">推荐系统学习（Word2vec）</a><time datetime="2025-04-15T09:27:44.000Z" title="发表于 2025-04-15 17:27:44">2025-04-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E9%80%89%E6%8B%A9%E6%80%A7%E5%81%8F%E5%B7%AE%EF%BC%89/" title="推荐系统学习（选择性偏差）">推荐系统学习（选择性偏差）</a><time datetime="2025-04-11T07:55:24.000Z" title="发表于 2025-04-11 15:55:24">2025-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8F%AC%E5%9B%9E%EF%BC%89/" title="推荐系统学习（召回）">推荐系统学习（召回）</a><time datetime="2025-04-11T03:57:13.000Z" title="发表于 2025-04-11 11:57:13">2025-04-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 舲.</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>